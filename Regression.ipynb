{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr7-j3Dz2fLM"
      },
      "source": [
        "<!-- JPN -->\n",
        "# 線形回帰と多項式回帰\n",
        "\n",
        "※本演習資料の二次配布・再配布はお断り致します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxfBDa0FkARI"
      },
      "source": [
        "<!-- ENG -->\n",
        "# Linear and Polynomial Regressions\n",
        "\n",
        "※Distribution or redistribution of these exercise materials without the copyright holder’s permission is not permitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR-Nczy5DGUH"
      },
      "source": [
        "<!-- JPN -->\n",
        "　回帰問題とは、例えば「2月24日（水）18時、予想気温6度の時の東京都目黒区の電力需要は何万キロワットか予想せよ」というように、**いくつかの要素を元に、実数値を予測する問題**のことを指す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETA1NPwVkARK"
      },
      "source": [
        "<!-- ENG -->\n",
        "　A regression problem is **a problem that predicts a real number based on several factors**, for example, \"Predict how many thousands of kilowatts the electricity demand will be in Meguro-ku, Tokyo, at 6:00 p.m. on Wednesday, February 24, when the expected temperature is 6 degrees Celsius\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdvcR40md4S"
      },
      "source": [
        "<!-- JPN -->\n",
        "　本日の演習では、3つの回帰予測を行うための機械学習アルゴリズムを説明する。\n",
        "\n",
        "- **1 | 線形回帰 (Linear regression)**\n",
        "- **2 | 多項式回帰 (Polynomial regression)**\n",
        "- **3 | 過剰適合を防ぐ正則化（Ridge回帰）**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfFUyBdnkARM"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In today's exercise, we will describe a machine learning algorithm for making three regression predictions.\n",
        "\n",
        "- **1 | Linear regression**\n",
        "- **2 | Polynomial regression**\n",
        "- **3 | Norm regularization to prevent overfitting (Ridge regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEd9rJQGr9Y7"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 1 | 線形回帰 (Linear regression)\n",
        "\n",
        "　まず、$y = f(x_1, x_2)$ を、既知データ $X, y$ を利用することで線形回帰で推定することをおこなってみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cWMpcXkARR"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 1 | Linear regression\n",
        "\n",
        "　First, let's try to estimate $y = f(x_1, x_2)$ by linear regression using the known data $X, y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4h3uqHNpbd"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 1.1 | データセットの作成\n",
        "\n",
        "　1 で利用する仮想的なデータセットを作成する。\n",
        "ここでは、$y = 3x_1 - x_2 + 2$ という線形の関係を満たすような、ノイズが無い10件のデータ $X, y$ を作っている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEwtV9KrkARU"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 1.1 | Creating a data set\n",
        "\n",
        "　Create a hypothetical data set to be used in 1.\n",
        "Here, we are making a sample $X, y$ with a sample size of 10, no noise, such that the linear relationship $y = 3x_1 - x_2 + 2$ is satisfied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWejgLK-tDag",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # For drawing (also used in Exercises in Fundamentals of Data Science ②)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QndsdozmtIWG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a data set\n",
        "n_data = 10                   # Number of data sets to be created\n",
        "X = np.random.rand(n_data, 2) # Randomly generate n_data sets for x\n",
        "y = 3 * X[:, 0] - X[:, 1] + 2\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjIq8XNPO2s5"
      },
      "source": [
        "<!-- JPN -->\n",
        "　このコードで注意する点を以下にまとめておく。NumPyの使い方にも関わるので、しっかり確認しておこう。\n",
        "* $X, y$ ともに10件のデータをまとめて表現している。\n",
        "* $X$ は、1つのデータ $\\boldsymbol{x}^T=[x_1, x_2]$ が**1つの行ベクトル**として、10件縦に重なって行列を構成している。（講義資料を読み返そう。確かに $X$ は $\\boldsymbol{x}^T$ を縦に重ねたものになっている）\n",
        "* Pythonでは添え字は0から始まるので、10件のデータ全ての $x_1$ を取得する、という操作は`X[:, 0]`に対応する。\n",
        "  * このため、 `y = 3*X[:, 0] - X[:, 1] + 2` と記述することで10件のデータの $y$ を同時に作ることができる。\n",
        "  * `X[:, 0]`は**すべて（`:`）の行の、0列目のデータを取得する**という意味であると考えよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjuFdxCkARX"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The following is a summary of points to aware of in this code. Since it is related to the usage of NumPy, let's check it carefully.\n",
        "* $X, y$ are both represented as a set of 10 data items.\n",
        "* $X$ is **a single row vector** of data $\\boldsymbol{x}^T=[x_1, x_2]$, which is vertically stacked 10 times to form a matrix. (Let's read the lecture materials again. Indeed, $X$ is stacked vertically with $\\boldsymbol{x}^T$ \n",
        "* In Python, an index starts from 0, so the operation to get $x_1$ for all 10 data items corresponds to `X[:, 0]`.\n",
        "  * Therefore, we can create 10 sets data for $y$ simultaneously by writing `y = 3*X[:, 0] - X[:, 1] + 2`.\n",
        "  * Consider that `X[:, 0]` means to **get the data of the 0th column of all (`:`) rows**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okd_ZQrbI0cL"
      },
      "source": [
        "<!-- JPN -->\n",
        "  なお、説明の都合上、変数 $\\boldsymbol{x}$ の添え字が講義資料と異なる意味で使われている場所がある。注意して読み進めてほしい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjnpbZ3BkARX"
      },
      "source": [
        "<!-- ENG -->\n",
        "  For the convenience of explanation, the index of the variable $\\boldsymbol{x}$ is used in some places with a different meaning than in the lecture material. Read and proceed with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyHz-vAv_-Ow"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 1.2 | `scikit-learn` を利用した線形回帰\n",
        "\n",
        "　まず、線形回帰を `scikit-learn` というPythonの機械学習ライブラリを用いて実行してみよう。 `scikit-learn` は機械学習の定番と言っても良いライブラリであり、今後も基盤データサイエンス演習、基盤人工知能演習 いずれも多用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVlxI2jTkARY"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 1.2 | Linear regression using `scikit-learn`\n",
        "\n",
        "　First, let's run linear regression using the Python machine learning library called `scikit-learn`. `scikit-learn` is a famous library for machine learning, and we will continue to use it for both Exercises in Fundamentals of Data Science and Exercises in Fundamentals of Artificial Intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2aHFAjUOQS"
      },
      "source": [
        "<!-- JPN -->\n",
        "　`scikit-learn` で用意されている関数は極めて賢く作られており、定数項の推定のための変数の追加などは内部で実行される。 `scikit-learn` で学習を行う際には、以下の順番で計算を行う。\n",
        "1.   機械学習アルゴリズム（今回は線形回帰）を選択する。\n",
        "2.   `fit()` を実行することでモデルを構築する。\n",
        "3.   必要に応じて `model.coef_` や `model.intercept_` を使って線形モデルの各説明変数に対する重み、および定数項を確認する。\n",
        "\n",
        "以下で実際にやってみよう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0CWWx4dkARZ"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The functions provided by `scikit-learn` are extremely intelligent, and adding variables for estimating constant terms is carried out internally. When learning with `scikit-learn`, calculations are performed in the following order.\n",
        "1. Select a machine learning algorithm (in this case, linear regression).\n",
        "2. Build a model by running `fit()`.\n",
        "3. Check the weights and constant terms for each explanatory variable in the linear model using `model.coef_` and `model.intercept_` as needed.\n",
        "\n",
        "Let's try to put it into practice as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EucmIMdDINrw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgOc9rRZAMR4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 1. Selecting a machine learning algorithm\n",
        "# As if you had prepared the equation for the linear model y = w^t x + b\n",
        "model = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWNjAyHxIIXx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 2. Building a linear model by running fit(), internally calculating 1.3\n",
        "# X is entered without adding a constant term\n",
        "# Input y as a one-dimensional array without vectorization\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7uxu8scIKQ3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 3. Checking the weights\n",
        "print(model.coef_)      # Check the values of w_1 and w_2\n",
        "print(model.intercept_) # Check the value of b (w_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6liPm5l9HsT"
      },
      "source": [
        "<!-- JPN -->\n",
        "　`model.fit()` に入れる `X`, `y` はそれぞれ行列と1次元配列であることに注意してほしい。\n",
        "\n",
        "　printされた結果を見ると、$\\boldsymbol{\\hat{w}} = [3, -1], \\hat b = 2$ という結果が得られ、正しく $y = 3x_1 - x_2 + 2$ を推定できていることがわかる。\n",
        "\n",
        "　今回はデータにノイズが存在せず、 $x$ と $y$ の関係が線形で線形回帰モデルが表現できる関係になっているので、完全に予測することができている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG7X8FTVkARa"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Note that the `X`, `y` in `model.fit()` are a matrix and a one-dimensional array, respectively.\n",
        "\n",
        "　The printed result shows that we have correctly estimated $y = 3x_1 - x_2 + 2$ with the result $\\boldsymbol{\\hat{w}} = [3, -1], \\hat b = 2$.\n",
        "\n",
        "　This time, there is no noise in the data, and the relationship between $x$ and $y$ is linear, a relationship that can be represented by a linear regression model, so it is perfectly predictable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Navya_K3VUCv"
      },
      "source": [
        "<!-- JPN -->\n",
        "　次に、このモデルを使って新しいデータ $(x_1, x_2) = (10, 30), (2, 1)$ を予測してみよう。新しいデータへの予測は `predict()` メソッドを利用する。\n",
        "$y = 3x_1 - x_2 + 2$ なので、この2つはそれぞれ $y = 2, 7$ となるはずである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lv3d6jWkARf"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Next, let's use this model to predict the new data $(x_1, x_2) = (10, 30), (2, 1)$. Prediction for new data is performed using the `predict()` method.\n",
        "Since $y = 3x_1 - x_2 + 2$, these two should be $y = 2, 7$ respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86QHUzbU-Kzs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a matrix of multiple data as we did for X.\n",
        "X_new = np.array([[10, 30],\n",
        "                  [2, 1]])\n",
        "model.predict(X_new) # Note that the prediction results will also come out as a one-dimensional array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBSSVcJlVmWH"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これで、新しいデータに対する予測を行うことができた。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0z7B03kARf"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This allowed us to make predictions for new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDhTBz7PNjXX"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 1.3 | NumPyを利用した線形回帰\n",
        "\n",
        "　通常の教材などであれば、scikit-learnによる線形回帰ができればそれでおしまい…なのだが、この演習は**講義で学んだものを実際に使う**ことも1つの目的である。本日講義で学んだ**線形代数の計算に基づいて** $y = \\boldsymbol{w}^T\\boldsymbol{x} + b = w_1x_1 + w_2x_2 + b$ の $\\boldsymbol{w}, b$ の推定を行い、scikit-learnと同一の結果が確かに得られることを確認しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBp9aJM8kARg"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 1.3 | Linear regression using NumPy\n",
        "\n",
        "　If it were the regular course materials, we would be able to do linear regression just using scikit-learn and that would be the end of it... However, one of the purposes of this exercise is to **actually use what we have learned in the lecture**. **Based on the linear algebra calculations** we learned in today's lecture, let's estimate $\\boldsymbol{w}, b$ for $y = \\boldsymbol{w}^T\\boldsymbol{x} + b = w_1x_1 + w_2x_2 + b$, and confirm that the results are indeed identical to scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8-2p3ddygAT"
      },
      "source": [
        "<!-- JPN -->\n",
        "　まず、定数項 $b$ の処理を簡単にするため、全ての要素が1である $x_3$ を `X` に追加したものを `X_aux` として定義する。これを行うことで、 $y = \\boldsymbol{\\tilde{w}}^T\\boldsymbol{\\tilde{x}} = w_1x_1 + w_2x_2 + w_3x_3 = w_1x_1 + w_2x_2 + w_3$ として表現することができ、 $b$ を $\\boldsymbol{\\tilde{w}}$ に含めることができる。（以後、 $\\boldsymbol{\\tilde{w}}$ のことを単に $\\boldsymbol{w}$ と記述する。）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-BIHm6jkARg"
      },
      "source": [
        "<!-- ENG -->\n",
        "　First, to simplify the process of the constant term $b$, we define as `X_aux` the addition of $x_3$, where all elements are 1, to `X`. By doing this, we can express it as $y = \\boldsymbol{\\tilde{w}}^T\\boldsymbol{\\tilde{x}} = w_1x_1 + w_2x_2 + w_3x_3 = w_1x_1 + w_2x_2 + w_3$ and include $b$ in $\\boldsymbol{\\tilde{w}}$. (Hereafter, $\\boldsymbol{\\tilde{w}} $ will be referred to simply as $\\boldsymbol{w}$.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-2GUz0ryv5H",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ones = np.ones((10,1))\n",
        "X_aux = np.hstack([X, ones]) # Assign x_3=1 to each data set\n",
        "print(X_aux)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScesrJWZSYOG"
      },
      "source": [
        "<!-- JPN -->\n",
        "　それでは、この $\\boldsymbol{w}$ を10件のデータから推定しよう。講義資料によれば、$\\boldsymbol{\\hat w} = (X^TX)^{-1}X^T\\boldsymbol{y}$ を計算することで、最小二乗法による $\\boldsymbol{w}$ の推定が行えるので、これを計算してみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yukA5dhkARh"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Now let's estimate this $\\boldsymbol{w}$ from the 10 data items. According to the lecture materials, we can estimate $\\boldsymbol{w}$ using the least-squares method by calculating $\\boldsymbol{\\hat w} = (X^TX)^{-1}X^T\\boldsymbol{y}$, so let's calculate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEwqOGLnFB_j",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "Y = y.reshape(-1, 1) # Since this is a linear algebra calculation, it should be column vectorized.\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKu_slGMwSEi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def estimate_parameters(X_train, Y_train):\n",
        "\n",
        "  XtX = np.dot(X_train.T, X_train)\n",
        "  XtXinvXt = np.dot(np.linalg.inv(XtX), X_train.T)\n",
        "  return np.dot(XtXinvXt, Y_train)\n",
        "\n",
        "w_hat = estimate_parameters(X_aux, Y)\n",
        "print(w_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wih6s7ctEVso"
      },
      "source": [
        "<!-- JPN -->\n",
        "　$\\boldsymbol{\\hat w} = [3, -1, 2]^T$ という結果が得られ、scikit-learnを使った場合と全く同じ結果が得られることが確認できた。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XS-S3jXkARi"
      },
      "source": [
        "<!-- ENG -->\n",
        "　We get the result $\\boldsymbol{\\hat w} = [3, -1, 2]^T$, which is exactly the same as the result we get using scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pgE61UvSGuf"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJRf0gB-Dizc"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 1\n",
        "\n",
        "　算出された $\\boldsymbol{\\hat w}$ を用いて $X = [[5, 1], [2, 4]]$ に対する $\\boldsymbol{\\hat y} = X\\boldsymbol{\\hat w}$ の予測を行うことを考える。\n",
        "以下のコードの `__xxxxx__` 部を埋めて、 $\\boldsymbol{\\hat y}$ を算出せよ（今回は $\\boldsymbol{\\hat w}$ が誤差なく推定出来ているので、$\\boldsymbol{\\hat y}$ は $y = 3x_1 - x_2 + 2$ に一致するはずである）。 \n",
        "\n",
        "　なお、課題提出時には以下の2つを記述せよ。\n",
        "* `__xxxxx__` に何を記述したか（NumPyを用いて、scikit-learnは利用しないこと）\n",
        "* `print(y_hat)` の出力結果（Pythonが**整数値**として処理している場合は整数値を、**実数値**として処理している場合は小数点以下第1位まで答えよ）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hG4WkKwkARi"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 1\n",
        "\n",
        "　Consider using $\\boldsymbol{\\hat w}$ that was computed to predict $\\boldsymbol{\\hat y} = X\\boldsymbol{\\hat w}$ for $X = [[5, 1], [2, 4]]$.\n",
        "Calculate $\\boldsymbol{\\hat y}$ by filling in the `__xxxxx__` part of the following code (since we were able to estimate $\\boldsymbol{\\hat w}$ without error this time, $\\boldsymbol{\\hat y}$ should match $y = 3x_1 - x_2 + 2$). \n",
        "\n",
        "　When you submit your exercise assignment, please include the following two items.\n",
        "* What you wrote in `__xxxxx__`? Answer with use of NumPy functions. (Do not use scikit-learn functions.)\n",
        "* `print(y_hat)` output result (If Python treats it as **an integer value**, answer it as an integer value, and if it treats it as **a real number**, answer it to the first decimal place)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ3aRM2h_ZZY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "w_hat = np.array([[3, -1, 2]]).T\n",
        "new_X = np.array([[5,1], [2,4]])           # Predict two sets of data [5,1] and [2,4]\n",
        "new_X = np.hstack([new_X, np.ones((2,1))]) # Add x_3 to new_X as well\n",
        "print(new_X)\n",
        "y_hat = __xxxxx__\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwUIKHeTT_2Y"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 1</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf-jHfmZVhUK"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "\n",
        "```\n",
        "__xxxxx__ = np.dot(new_X, w_hat), new_X.dot(w_hat) など、new_Xとw_hatの内積\n",
        "y_hat     = [[16.0],[4.0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbOvAzpDU3V"
      },
      "source": [
        "---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfNwrXvrcMk1"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 2 | 多項式回帰 (Polynomial regression) \n",
        "　次に、$y = \\sin(x)$ を多項式 $\\hat f(x) = \\sum_{i=0}^p \\hat w_i x^i$ で近似してみることを考える。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YUwhZ0kARj"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 2 | Polynomial regression \n",
        "　Next, consider approximating $y = \\sin(x)$ with the polynomial $\\hat f(x) = \\sum_{i=0}^p \\hat w_i x^i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1JGQ8UeWnxY"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.1 | データセットの作成\n",
        "\n",
        "　まずは、先ほどと同様に仮想的なデータを10個、ランダムに作成する。\n",
        "ただし、今回のデータは 第1章で利用した線形回帰のデータとは異なり、 $y$ の観測誤差が平均 0.1 程度含まれているものとする。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd3sTOHWkARj"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.1 | Creating a data set\n",
        "\n",
        "　First, create 10 hypothetical data sets as before, randomly.\n",
        "However, this data is different from the linear regression data used in Section 1, and it is assumed that the observation error of $y$ is included about 0.1 on average.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXFIt7GGQ9qa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # For drawing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PF8OsO0IYD9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# create toy data set\n",
        "np.random.seed(7) # A trick that allows you to create the same random data every time.\n",
        "\n",
        "n_data = 10\n",
        "x = 6 * np.random.rand(n_data) - 3         # Randomly generate 10 values ​​from -3 to 3\n",
        "noise = 0.1 * np.random.randn(n_data)      # Noise\n",
        "y = np.sin(x) + noise                      # Calculate y = sin (x) + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epbKIUjUXQLd"
      },
      "source": [
        "<!-- JPN -->\n",
        "　先ほどの線形回帰のデータと異なり、作成された **$x, y$ はどちらも1次元配列である**ことに注意しよう。作成したデータをプロットしてみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-AwRyZ9kARk"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Note that unlike the previous linear regression data, **$x, y$ that were created are both one-dimensional arrays**. Let's plot the data we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nObimhW-HaiU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# In order to draw a graph of y = sin (x) smoothly,\n",
        "# calculate the value of sin in increments of 0.01\n",
        "# and display it as a line graph\n",
        "xg = np.arange(-3, 3, 0.01)\n",
        "yg = np.sin(xg)\n",
        "plt.plot(xg, yg, \"red\", label=\"ground truth\")\n",
        "\n",
        "# Display the created data as a scatter plot diagram\n",
        "plt.scatter(x, y, label=\"observed data\")\n",
        "\n",
        "plt.legend(loc = \"lower right\") # Display legend\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ERkKbRXXbg"
      },
      "source": [
        "<!-- JPN -->\n",
        "　基本的には $y = \\sin(x)$ に従っているが、 $y$ に関する観測誤差のためにわずかに上下に値がずれている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7WPbnYUkARk"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Basically, it follows $y = \\sin(x)$, but the values ​​are slightly varied up and down due to the observation error regarding $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2oovc7aX4fx"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.2 | scikit-learnを利用した多項式回帰\n",
        "　それでは、多項式回帰を行うことで、 $\\sin(x)$ を3次関数 $\\hat f(x) = \\hat w_0 + \\hat w_1x^1 + \\hat w_2x^2 + \\hat w_3x^3$ で近似してみる。ここで **$[s, t, u] = [x^1, x^2, x^3]$** という3つの説明変数を考えると、**3変数の線形回帰と全く同じ式**に帰着することができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP5zRKL8kARk"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.2 | Polynomial regression using scikit-learn\n",
        "　Now, let's approximate $\\sin(x)$ by doing a polynomial regression with the cubic function $\\hat f(x) = \\hat w_0 + \\hat w_1x^1 + \\hat w_2x^2 + \\hat w_3x^3$. If we consider the three explanatory variables, **$[s, t, u] = [x^1, x^2, x^3]$**, we can arrive at **the exact same equation as the linear regression of the three variables**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evNjB-svkARl"
      },
      "source": [
        "![Figure 1](https://i.imgur.com/WuOonqI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZmb6zDzm-e0"
      },
      "source": [
        "<!-- JPN -->\n",
        "**線形回帰と多項式回帰の関係**  定数項は $w_0$ で表現している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiQ5GpJUkARl"
      },
      "source": [
        "<!-- ENG -->\n",
        "**Relationship between linear and polynomial regression** The constant term is represented by $w_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUKPFP-DqeV5"
      },
      "source": [
        "<!-- JPN -->\n",
        "　このことから、scikit-learnでは、以下の手順で3次多項式の回帰を実現する。\n",
        "\n",
        "1. `PolynomialFeatures()` クラスを利用して スカラー値 $x$ を 3 個の説明変数 $[x_1, x_2, x_3]^T = [x^1, x^2, x^3]^T$ に拡張する。\n",
        "2. $[x_1, x_2, x_3]^T$ を入力として、線形回帰 `LinearRegression()` を行うことで、 $\\boldsymbol{\\hat{w}} = [\\hat w_1, \\hat w_2, \\hat w_3]^T, \\hat b$ を推定する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy1jcxX2kARl"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Therefore, in scikit-learn, the following procedure is used to achieve cubic polynomial regression.\n",
        "\n",
        "1. Use the `PolynomialFeatures()` class to extend the scalar value $x$ to three explanatory variables $[x_1, x_2, x_3]^T = [x^1, x^2, x^3]^T$.\n",
        "2. Take $[x_1, x_2, x_3]^T$ as input and estimate $\\boldsymbol{\\hat w}^T = [\\hat w_1, \\hat w_2, \\hat w_3]^T, \\hat b$ by performing `LinearRegression()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDMnl0yqBhDp"
      },
      "source": [
        "<!-- JPN -->\n",
        "　それでは早速 `PolynomialFeatures()` を使ってみよう。 `PolynomialFeatures()` は、以下のような基底関数 $\\phi_i(x)$ の組を使って 1 変数の値を $p+1$ 変数の値に拡張する（正確には2変数以上の値も処理できるが、ここでは説明を割愛する）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGRuGVaSkARm"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Now let's try using `PolynomialFeatures()`. `PolynomialFeatures()` extends the value of one variable to the value of $p+1$ variables using a pair of basis functions $\\phi_i(x)$ as follows (to be precise, it can handle more than two variables, but I will not explain it here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ikjAAfPMTEj"
      },
      "source": [
        "$$ \n",
        "\\phi(x) = \\left[ \n",
        "  \\begin{array}{c}\n",
        "    \\phi_0(x)\\\\\n",
        "    \\phi_1(x)\\\\\n",
        "    \\vdots\\\\\n",
        "    \\phi_p(x)\n",
        "  \\end{array}\n",
        "\\right] = \\left[\n",
        "  \\begin{array}{c}\n",
        "    1\\\\\n",
        "    x\\\\\n",
        "    \\vdots\\\\\n",
        "    x^p\n",
        "  \\end{array}\n",
        "\\right]\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBsncYU1KdDb"
      },
      "source": [
        "<!-- JPN -->\n",
        "　ただし、これまでに説明したように `LinearRegression()` はそれ自体が定数項を追加するため、`PolynomialFeatures`が定数項を作成してしまうのは好ましくない。このような時は、 `PolynomialFeatures(include_bias=False)` と指定すれば、定数関数 $\\phi_0(x) = x^0 = 1$ を除外して $p$ 変数の値に拡張することができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU3qFxnzLK8K"
      },
      "source": [
        "<!-- ENG -->\n",
        "　As mentioned before, `LinearRegression()` itself adds a constant term, and thus it is not desirable that `PolynomialFeatures` creates a constant term. In such a case, you can specify `PolynomialFeatures(include_bias=False)` to exclude the constant function $\\phi_0(x) = x^0 = 1$ and extend one variable $x$ to the $p$ variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7JMusU4MWEK"
      },
      "source": [
        "<!-- JPN -->\n",
        "`PolynomialFeatures()` 自体も入力として行列 $X$ を受け取るものになっている。2.1節で作成した $\\boldsymbol{x}$ は行列になっていないので、行列表現に変換してから、 `PolynomialFeatures()` を利用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyCg3Z4qkARm"
      },
      "source": [
        "<!-- ENG -->\n",
        "`PolynomialFeatures()` itself takes a matrix $X$ as input. Since the $\\boldsymbol{x}$ created in Section 2.1 is not a matrix, we will convert it to a matrix representation and then use `PolynomialFeatures()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgq_m4fUpx93",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(x)  # The original data is a one-dimensional array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxwO_spzaaAF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X = x.reshape(-1, 1) # Make a data matrix with N vertically arranged data consisting of a single feature.\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIb1C8qLB6_K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "p = 3 # Use up to the 3rd order\n",
        "\n",
        "# It can be converted by running fit_transform()\n",
        "X_poly = PolynomialFeatures(degree=p, include_bias=False).fit_transform(X)\n",
        "print(X_poly) # Note that the order is x^1, x^2, x^3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPX-vrHua7MB"
      },
      "source": [
        "<!-- JPN -->\n",
        "　なお、`1.00e+01`とは、$1.00 \\times 10^{1} = 10.0$ という意味である。あとは、これを使って `LinearRegression()` をすれば、多項式回帰が行えるはずである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjrJGndZkARn"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Note that `1.00e+01` means that $1.00\\times 10^{1} = 10.0$. Next, we can use this to do `LinearRegression()`, and thus we should be able to carry out polynomial regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvfiFXVASS_Y"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC646SYUC1Ku"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 2\n",
        "\n",
        "　上記の特徴量を用いて3.1節と同様に線形回帰を行うことで、多項式回帰が実現される。\n",
        "1.2節で実行した線形回帰のコードを参考に、`LinearRegression()` を用いて線形モデルを構築し、`model.coef_` および `model.intercept_` を出力せよ。\n",
        "\n",
        "　なお、課題提出時には、`model.coef_` および `model.intercept_` の結果を、**小数点以下第4桁を四捨五入した値**をレポートに記述せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrrQqp62kARn"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 2\n",
        "\n",
        "　Polynomial regression is performed by performing linear regression as in Section 3.1 using the above feature.\n",
        "Using the linear regression code you ran in Section 1.2 as a reference, build a linear model using `LinearRegression()` and output `model.coef_` and `model.intercept_`.\n",
        "\n",
        "　When you submit your exercise assignment, write the results of `model.coef_` and `model.intercept_`, **rounded off to three decimal places**, in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8zcW4fFNMY_"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4NsdJXPM4al"
      },
      "outputs": [],
      "source": [
        "# ANS\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "print(model.coef_)\n",
        "print(model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbd3k3XLSVx-"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ri0mXzZE1lh"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 3\n",
        "\n",
        "　課題 2 の結果を解釈しよう。推定された3次関数式 $\\hat f(x)$ を答えよ。ただし、課題 2 と同様に、係数は全て**小数点以下第4位を四捨五入し**、`f(x) = ax^3 + bx^2 + cx + d` の形式でレポートに記述せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7DzbIlKkARn"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 3\n",
        "\n",
        "　Interpret the results of exercise 2. Answer the estimated cubic function equation $\\hat f(x)$. However, as in exercise 2, **round off all coefficients to three decimal places**, and write your report in the form `f(x) = ax^3 + bx^2 + cx + d`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6AZnb97NW14"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 3</font>\n",
        "\n",
        "上記 課題 2の結果から、 $f(x) = 0.098 x^3 + 0.013 x^2 - 0.888 x - 0.020$ が答えとなる。`y=sin(x)` の `x=0` 近傍におけるテイラー展開を考えて、係数の順序などが適切であることを確認すると良い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV6fmvUSTo-A"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUr_-yMDKZP"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.3 | `make_pipeline()` を利用したモデルの構築\n",
        "\n",
        "　今回の予測は、「多項式の特徴量作成」と「線形回帰」という複数のステップを踏んだ。このような場合、いちいち途中結果を出力することなく、複数のステップを1つにまとめたモデルを作る`make_pipeline()`が利用可能である。ただし、`make_pipeline()`を利用すると重みを取り出すためにひと手間必要になるので注意が必要だ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvalGU5hkARo"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.3 | Building a model using `make_pipeline()`\n",
        "\n",
        "　Our prediction so far took multiple steps that are \"polynomial feature generation\" and \"linear regression.\" Alternatively, we can use `make_pipeline()` which constructs a model by combining multiple steps into one without having to output the results of each step. Note, however, that if you use `make_pipeline()`, you will need to do a little work to get the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHENcnLvE7Wv",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "p = 3 # Use up to the 3rd order\n",
        "\n",
        "# Input make_pipeline (1st process, 2nd process, ...) in the order being processed.\n",
        "model_polyreg = make_pipeline(\n",
        "    PolynomialFeatures(degree=p, include_bias=False),\n",
        "    LinearRegression())\n",
        "\n",
        "# You can simply use fit() even when processing multiple calculations\n",
        "X = x.reshape(-1, 1)\n",
        "model_polyreg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGO74LJDcs08",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Since LinearRegression is the second element of Pipeline,\n",
        "# access the contents of LinearRegression as model_polyreg[1].\n",
        "print(model_polyreg[1].coef_)\n",
        "print(model_polyreg[1].intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzK2i5YTd8RM"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.4 結果の描画とパラメータ $p$ の調整\n",
        "\n",
        "　ところで予測された $\\hat f(x)$ はどれほど $\\sin(x)$ に似ているだろうか。グラフを描画して確認してみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rh5DsAVkARp"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.4 Drawing the results and adjusting parameter $p$\n",
        "\n",
        "　How similar is the predicted $\\hat f(x)$ to $\\sin(x)$, by the way? Check it by drawing a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwpS8LJfDt5p",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Function to do all drawings\n",
        "# x, y are one-dimensional arrays\n",
        "def draw(x, y, model):\n",
        "  # Scatter plot diagram of data points used for machine learning\n",
        "  plt.scatter(x, y, label=\"observed data\")\n",
        "\n",
        "  # Draw sin(x)\n",
        "  xg = np.arange(-3, 3, 0.01)\n",
        "  yg = np.sin(xg)\n",
        "  plt.plot(xg, yg, \"red\", label=\"ground truth\")\n",
        "\n",
        "  # Draw the estimated f(x)\n",
        "  Xg = xg[:, np.newaxis] # Convert it into matrix\n",
        "  y_est = model.predict(Xg)\n",
        "  plt.plot(xg, y_est, \"blue\", label=\"estimation\")\n",
        "\n",
        "  # Plotting processing\n",
        "  plt.legend(loc = \"lower right\") # Display legend\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhXx68DLeUoo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Input the first data x, y and the model created with pipeline.\n",
        "draw(x, y, model_polyreg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPlw5e1DFnZJ"
      },
      "source": [
        "<!-- JPN -->\n",
        "　本来の関数が赤いグラフ、今回推定した $\\hat f(x)$ が青いグラフとなっている。かなり良い近似が得られているようだ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRLkhgA5kARq"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The original function is shown as a red graph, and the estimated $\\hat f(x)$ is shown as a blue graph. It seems we are getting a pretty good approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoieUagEesKc"
      },
      "source": [
        "<!-- JPN -->\n",
        "　さて、描画結果を見ることができるようになり、予測結果の良しあしを（主観的ながら）考えることができるようになったので、近似する多項式の次数 $p$ の値を変更させてみて、どのように関数形状が変化するか確認してみる。$p = 1$ から $p = 9$ まで変更させながら、関数の形状が本来の関数とどの程度類似するかを確認してみよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hGBSlDkkARq"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Now that we can see the results of the drawing, we can consider (subjectively) whether the prediction is good or bad, so let's try changing the value of order $p$ of the polynomial to be approximated and see how the function shape changes. Observe how similar the shape of the function is to that of the original function as you change from $p = 1$ to $p = 9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTi8QsRNQEpt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "p = 6 # This is the order of the polynomial. Let's try to vary it from one to nine\n",
        "\n",
        "model_polyreg = make_pipeline(\n",
        "    PolynomialFeatures(degree=p, include_bias=False),\n",
        "    LinearRegression())\n",
        "X = x.reshape(-1, 1)\n",
        "model_polyreg.fit(X, y)\n",
        "draw(x, y, model_polyreg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gey4GXVP0mC"
      },
      "source": [
        "<!-- JPN -->\n",
        "　多項式の次数 $p$ が増えるに従い推定された関数 (estimation) と真の関数 (ground truth) の形状が似てくるのだが、$p = 5$ あたりから推定された関数の形状が $\\sin(x)$ とかみ合わなくなり始め、 $p = 6$ 以降では全く関数形状が推定できなくなってしまう。\n",
        "\n",
        "　$p = 6$の場合、3次多項式の要素 $x^1, x^2, x^3$ は全て含まれており、3次多項式と同程度以上の高精度な予測が期待できるはずなのだが、$x^4, x^5, x^6$ の情報を使って観測データをより詳細に、**観測誤差までも**予測しようとし、結果として元の関数の形状を推定できなくなっている。これは**過剰適合(overfitting, 過学習ともいう)** と呼ばれる現象であり、**データ数が少なく、説明変数が多い時**に特に発生しやすい（**補足資料 ※1**）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMO3i34kARq"
      },
      "source": [
        "<!-- ENG -->\n",
        "　As the order $p$ of the polynomial increases, the shape of the estimated function (estimation) and the true function (ground truth) become more and more similar, but around $p = 5$, the shape of the estimated function starts to no longer match $\\sin(x)$, and after $p = 6$, the function shape cannot be estimated at all.\n",
        "\n",
        "　In the case of $p = 6$, all the elements $x^1, x^2, x^3$ of the cubic polynomial are included, and we should expect a prediction as accurate or better than the cubic polynomial, but the information of $x^4, x^5, x^6$ is also used to predict the observed data in more detail, **even the observation error**, and as a result, we cannot estimate the shape of the original function. This is a phenomenon called **overfitting**, which is especially likely to occur **when the number of data is small and the number of explanatory variables is large** (**Supplementary Material S1**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U166nPdJS6V-"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 3 | 過剰適合を防ぐ正則化（Ridge回帰; Ridge regression）\n",
        "\n",
        "　第2章の結果から、多項式の次元数 $p$ が非常に大きい時、多項式回帰の結果は過剰適合していることが分かった。\n",
        "これを避ける工夫として広く用いられているのが **Ridge回帰**である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt69uUbSkARr"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 3 | Norm regularization to prevent overfitting (Ridge regression）\n",
        "\n",
        "　In Section 2, we found that when the order $p$ of the polynomial is very large, the results of the polynomial regression are overfitted.\n",
        "**Ridge regression** is a widely used method to avoid this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyFofCImhR67"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 3.1 | scikit-learnの`Ridge()`を利用したRidge回帰の実施"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WHrZPg1kARr"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 3.1 | Performing ridge regression using `Ridge()` in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFLmlCq_h2_O"
      },
      "source": [
        "<!-- JPN -->\n",
        "　Ridge回帰はscikit-learnに`Ridge()`として定義されているので、これを利用して過剰適合を抑えてみる。 `Ridge()` には L2 正則化項の重み `alpha` をパラメータとして与える必要がある。`alpha` の値が大きいほど、過剰適合を抑えることができるはずだ。\n",
        "\n",
        "　以下のコードの `alpha = 0.1` の部分を様々な値に変更させながら、推定された関数形状を確認してみよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1pywfsZkARs"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Ridge regression is defined in scikit-learn as `Ridge()`, so we will try to use it to prevent overfitting. `Ridge()` needs to be given the weight `alpha` of the L2 norm regularization term as a parameter. The higher the `alpha` value, the less overfitting there is.\n",
        "\n",
        "　Let's check the estimated function shape while changing the `alpha = 0.1` part of the following code to various values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-9U0lJLjAl7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "p = 6\n",
        "alpha = 0.1\n",
        "\n",
        "model_poly_ridge = make_pipeline(\n",
        "    PolynomialFeatures(degree=p, include_bias=False), \n",
        "    Ridge(alpha=alpha))\n",
        "X = x.reshape(-1, 1) \n",
        "model_poly_ridge.fit(X, y)\n",
        "draw(x, y, model_poly_ridge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY6HcnXtnEVp"
      },
      "source": [
        "<!-- JPN -->\n",
        "　どうだろうか。正しく機能しているだろうか。**実はこれだけではなかなかうまくいかない**。\n",
        "\n",
        "　Ridge回帰の式を再度考えてみよう。Ridge回帰では、**損失関数**（今回の場合は平均二乗誤差）**と、モデルの複雑さを制御する L2 正則化項** (L2 norm regularization term) **の和が最小になるように重み $\\boldsymbol{w}$ を決定**する。（「モデルの複雑さ」については、今回の場合はグラフの荒ぶり度合いを想像すればよい）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nNtHwS9kARs"
      },
      "source": [
        "<!-- ENG -->\n",
        "　What do you think? Is it working properly?  **In fact, this is not quite enough to make it work**.\n",
        "\n",
        "\n",
        "\n",
        "　Let's consider the ridge regression equation again. In Ridge regression, **the weights $\\boldsymbol{w}$ are determined so that the sum of the loss function** (mean squared error in this case) **and the L2 norm regularization term that controls the complexity of the model is minimized**. (For \"model complexity\", in this case you can imagine from how rough the graph is)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1sxBOW2nF4-"
      },
      "source": [
        "$$\\frac{1}{n} \\sum^n_{i=1} l(f(\\boldsymbol{x}_{(i)}),y_i) + \\lambda||\\boldsymbol{w}||^2_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_oTv58Q-Ed_"
      },
      "source": [
        "<!-- JPN -->\n",
        "この式の中で $\\lambda |\\boldsymbol{w}|_2^2$ が正則化項である。この式から、正則化項の重み $\\lambda$ （sklearnでは`alpha`）を各重み **$w_i$ に対して均等に**効かせる。\n",
        "\n",
        "　一方、値の幅が大きい特徴量（今回の場合、$x^1$ よりも $x^6$ の方が値の幅（＝分散）が大きくなっているはずである）の重み $\\boldsymbol{w}$ は一般に小さくなるので、正則化項の重み $\\lambda$ (`alpha`) の効き目が弱くなってしまうのである（**補足資料 ※2**）。\n",
        "\n",
        "　このように、**値の幅が異なる説明変数に対してRidge回帰を適用する場合には、特徴量の値を平均0、分散1にそろえる標準化 (Standardization) を行う**と良い。標準化を行う場合は、`make_pipeline`の`Ridge`の前に`StandardScaler()`を導入する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FRKsHOrkARt"
      },
      "source": [
        "<!-- ENG -->\n",
        "In this equation, $\\lambda |\\boldsymbol{w}|_2^2$ is the norm regularization term. From this equation, the weight of the norm regularization term $\\lambda$ (`alpha` in scikit-learn) is applied **equally to each weight $w_i$**.\n",
        "\n",
        "　On the other hand, the weight $\\boldsymbol{w}$ of a feature with a large range of values (in this case, the range of values (= variance) should be larger for $x^6$ than for $x^1$) is generally small, so the effect of the weight $\\lambda$ (`alpha`) of the norm regularization term becomes weaker (**Supplementary Material S2**).\n",
        "\n",
        "　Thus, **when ridge regression is applied to explanatory variables with different value ranges, it is recommended to perform standardization to align the values of the features to mean 0 and variance 1.** To standardize, introduce `StandardScaler()` before the `Ridge` in `make_pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw_1zltG9TEW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "p = 6\n",
        "alpha  = 0.1\n",
        "\n",
        "model_poly_ridge = make_pipeline(\n",
        "    PolynomialFeatures(degree=p, include_bias=False), \n",
        "    StandardScaler(),\n",
        "    Ridge(alpha=alpha))\n",
        "model_poly_ridge.fit(X, y)\n",
        "\n",
        "draw(x, y, model_poly_ridge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHqTl32O-byQ"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これを行うことで、高次の項まで利用した場合でも、だいぶ本来の $y = \\sin(x)$ に近い関数形状を推定することができた。このように、Ridge回帰は過剰適合を抑えることができる。\n",
        "\n",
        "　ただしその一方で、この予測された関数は**3次多項式による近似に比べると関数形状の推定が僅かに悪い**ことには注意する必要がある。**予測すべき対象の関数の概形が既知**で、それに対して**適切な関数（モデル）が理論や人間の感覚から推定できる**のであれば、**それより複雑なモデルを導入しても通常良い結果はもたらさない**ということは、覚えておくと良いだろう。（**補足資料 ※3**）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEXYKj9okARu"
      },
      "source": [
        "<!-- ENG -->\n",
        "　By doing this, we were able to estimate a function shape that is much closer to the original $y = \\sin(x)$, even when using higher order terms. In this way, ridge regression can prevent overfitting.\n",
        "\n",
        "　On the other hand, it should be noted that this predicted function gives **a slightly worse estimation of the function shape than the approximation by a cubic polynomial**. It is worth remembering that if **the approximate form of the function to be predicted is known**, and **an appropriate function (model) for it can be estimated from theory or human intuition**, then **introducing a more complex model will not usually produce good results**. (**Supplementary Material S3**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQfaVZU1ShuR"
      },
      "source": [
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4BzF6qfGjCw"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 4\n",
        "\n",
        "　上記の資料では、Ridge回帰の `alpha` を0.1としていた。この値を $\\alpha = 1$ や $\\alpha = 10$ と大きくしていった場合にestimationのグラフはどう変化するだろうか。`StandardScaler()`を含めたRidge回帰パイプラインに対して様々な $\\alpha$ を適用し、簡潔に解答せよ。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7hk--IPkARu"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 4\n",
        "\n",
        "　In the above document, the ridge regression `alpha` was set to 0.1. How does the estimation graph change if this value is increased to $\\alpha = 1$ or $\\alpha = 10$? Apply various $\\alpha$ to the ridge regression pipeline, including `StandardScaler()`, and give a concise answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMc8or8Xdx3B"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 4</font>\n",
        "αが大きくなるにつれて正則化の重みが強くなるため、αを大きくするとestimationのグラフが平坦になり、正弦曲線から離れていく。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbbf1KceSkSm"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rk45udCPigt"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 5\n",
        "\n",
        "　課題 4 で示したように、 $\\alpha$ の値は予測結果に大きな影響を与え、かつ最適な $\\alpha$ の値はその時々によって変化してしまう。また、ハイパーパラメータの選択肢が膨大になった場合には主観的なモデル選択は困難である。\n",
        "そこで、基盤データサイエンス演習 第3回で学習した `GridSearchCV()` を用いて、**5-fold 交差検証法 (cross validation)** による $\\alpha \\in [10^{-4}, 10^{-3}, ..., 10^{0}, 10^1]$ のハイパーパラメータ探索を行い、この回帰問題における最適な $\\alpha$ の値を推定せよ。\n",
        "\n",
        "　なお、pipelineの中のモデルに対して`GridSearchCV()`を行うのは簡単ではないため（不可能ではない）、以下のコードを参考に、Ridge回帰部分のみに対して`GridSearchCV()`を実施せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6We2DU4kARv"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 5\n",
        "\n",
        "　As shown in Exercise 4, the value of $\\alpha$ has a significant impact on the prediction results, and the optimal value of $\\alpha$ can vary from time to time. In addition, subjective model selection is difficult when the choice of hyperparameters becomes significantly large.\n",
        "Using `GridSearchCV()` that we learned in the Exercises in Fundamentals of Data Science ③, perform an $\\alpha \\in [10^{-4}, 10^{-3}, ..., 10^{0}, 10^1]$ hyperparameter search using **5-fold cross validation**, and estimate the optimal value of $\\alpha$ for this regression problem.\n",
        "\n",
        "　Note that it is not easy (but not impossible) to perform `GridSearchCV()` on the models in the pipeline, so refer to the following code and perform `GridSearchCV()` only on the ridge regression part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQZwJIqw070n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Incomplete code\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "p = 6\n",
        "param_grid = _____________\n",
        "X_poly = PolynomialFeatures(degree=p, include_bias=False).fit_transform(X)\n",
        "X_poly_standardized = StandardScaler().fit_transform(X_poly)\n",
        "\n",
        "grid_search_ridge = GridSearchCV(__________)\n",
        "grid_search_ridge.fit(X_poly_standardized, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-7IL_UT8oq"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 5</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x96aAcQ14lm6",
        "outputId": "e98da133-b5c4-4900-8003-08a19ccdeef3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.001}\n"
          ]
        }
      ],
      "source": [
        "# ANS\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "p = 6\n",
        "param_grid = [\n",
        "    {\n",
        "        \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
        "     }\n",
        "]\n",
        "X_poly = PolynomialFeatures(degree=p, include_bias=False).fit_transform(X)\n",
        "X_poly_standardized = StandardScaler().fit_transform(X_poly)\n",
        "\n",
        "model = Ridge(fit_intercept=False)\n",
        "\n",
        "grid_search_ridge = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search_ridge.fit(X_poly_standardized, y)\n",
        "print(grid_search_ridge.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_OuZTSuSnCX"
      },
      "source": [
        "-------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45YT7oH0HgxO"
      },
      "source": [
        "<!-- JPN -->\n",
        "#### 課題 6（発展、提出対象ではありません）\n",
        "\n",
        "　講義資料を参考に、NumPyを用いて**Ridge回帰**と**特徴量の標準化**を実装し、予測されたグラフがほぼ同一になることを確認せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zck6TjBFkARv"
      },
      "source": [
        "<!-- ENG -->\n",
        "#### Exercise 6 (Advanced, no need to submit)\n",
        "\n",
        "　Using the lecture materials as a reference, implement **ridge regression** and **feature standardization** using NumPy, and verify that the predicted graphs are nearly identical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcw2KcnBdx3C"
      },
      "source": [
        "<!-- ANS-JPN -->\n",
        "#### <font color=red>解答例：課題 6</font>\n",
        "\n",
        "$\\boldsymbol{\\hat w} = (X^TX+\\lambda I)^{-1}X^T\\boldsymbol{y}$\n",
        "\n",
        "を解くことで$\\boldsymbol{\\hat w}$ を計算することができる。\n",
        "以下が実装例であり、$\\boldsymbol{\\hat w}$ がこの計算結果と `print(model_poly_ridge[2].coef_)` などで確認できるRidge回帰で一致することを確認することで計算結果が一致することが確認できる。\n",
        "\n",
        "また、以下のように`draw()`を自分で実装する必要があるが、グラフを描画することもできる。\n",
        "\n",
        "なお、予測時も特徴量の標準化を行わなければならないことに注意せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReFrlhlBdx3C"
      },
      "outputs": [],
      "source": [
        "# ANS\n",
        "\n",
        "def polynomialfeatures_np(X, degree):\n",
        "    ret = []\n",
        "    for i in range(1, degree+1):\n",
        "        ret.append(X ** i)\n",
        "    return np.hstack(ret)\n",
        "\n",
        "def standardize_np(X):\n",
        "    mu = np.mean(X, axis=0)\n",
        "    sigma = np.std(X, axis=0)\n",
        "    sigma = np.array(\n",
        "        [1 if x==0 else x for x in sigma])\n",
        "    #print(mu, sigma)\n",
        "    X_cent = X - mu # centralized\n",
        "    \n",
        "    return (X - mu) / sigma\n",
        "    \n",
        "def ridge_np(X_train, Y_train, alpha):\n",
        "    XtX = np.dot(X_train.T, X_train)\n",
        "    XtXpluslI = XtX + alpha * np.eye(*XtX.shape) \n",
        "    w_hat = np.dot(np.dot(np.linalg.inv(XtXpluslI), X_train.T), Y_train)\n",
        "    return w_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ahqdKC5SF8K",
        "outputId": "2a09d4c2-3f61-4acc-9729-e7cb2319c3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.16527841]\n",
            " [ 0.10244322]\n",
            " [-0.2335626 ]\n",
            " [ 0.03396557]\n",
            " [-0.50434915]\n",
            " [-0.08202628]\n",
            " [ 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# ANS\n",
        "\n",
        "X_aux = polynomialfeatures_np(X, degree=6)\n",
        "ones = np.ones((10,1))\n",
        "\n",
        "X_aux = np.hstack([X_aux, ones]) # Assign x_3=1 to each data set\n",
        "Y = y.reshape(-1, 1) # Since this is a linear algebra calculation, it should be column vectorized.\n",
        "\n",
        "w_hat = ridge_np(standardize_np(X_aux), Y, 0.1)\n",
        "print(w_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNvijP1wNP9_"
      },
      "outputs": [],
      "source": [
        "# ANS\n",
        "\n",
        "def draw_np(x, y, w_hat):\n",
        "  # Scatter plot diagram of data points used for machine learning\n",
        "  plt.scatter(x, y, label=\"observed data\")\n",
        "\n",
        "  # Draw sin(x)\n",
        "  xg = np.arange(-3, 3, 0.01)\n",
        "  yg = np.sin(xg)\n",
        "  plt.plot(xg, yg, \"red\", label=\"ground truth\")\n",
        "\n",
        "  # Draw the estimated f(x)\n",
        "  Xg = xg[:, np.newaxis] # Convert it into matrix\n",
        "  ones = np.ones(Xg.shape)\n",
        "  Xg_poly = polynomialfeatures_np(Xg, degree=6)\n",
        "  Xg_poly = np.hstack([Xg_poly, ones])\n",
        "  Xg_poly = standardize_np(Xg_poly)\n",
        "  y_est = np.dot(Xg_poly, w_hat)\n",
        "  plt.plot(xg, y_est, \"blue\", label=\"estimation\")\n",
        "\n",
        "  # Plotting processing\n",
        "  plt.legend(loc = \"lower right\") # Display legend\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qMK9Dq5fN5MT",
        "outputId": "b71afe13-30fd-4a78-c703-a33e04d7864a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ANS\n",
        "\n",
        "draw_np(x, y, w_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wf0h27fj1zV"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfoHGg_YWyto"
      },
      "source": [
        "<!-- JPN -->\n",
        "# レポート提出について（2021年度版）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTuy6MF4_qRU"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "# Report submissions (FY2021)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leFjB4pNWyto"
      },
      "source": [
        "<!-- JPN -->\n",
        "## レポートの提出方法\n",
        "\n",
        "　レポートは答案テンプレートを参考に、**T2SCHOLA**上の「小テスト」機能を使うこと。**期限は次回の基盤人工知能演習の開始時刻まで**で、それ以後は一切の提出を受け付けないので時間に余裕をもって提出を行うこと。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fu336fn_qRV"
      },
      "source": [
        "<!-- ENG -->\n",
        "## How to submit reports\n",
        "\n",
        "　The report should be submitted **via T2SCHOLA**. **The deadline is the start of the next Exercise in the Fundamentals of Artificial Intelligence**. Be sure to submit your work with plenty of time to spare as the form cannot receive after the deadline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USqypDsXXKm9"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 答案テンプレート\n",
        "\n",
        "```\n",
        "学籍番号:\n",
        "名前:\n",
        "\n",
        "課題 1\n",
        "__xxxxx__ = __________________\n",
        "y_hat     = __________________\n",
        "\n",
        "課題 2\n",
        "model.coef_ = __________________\n",
        "model.intercept_ = __________________\n",
        "\n",
        "課題 3\n",
        "f(x) = _____x^3 + _____x^2 + _____x + _____\n",
        "（適宜符号を書き換えよ）\n",
        "\n",
        "課題 4\n",
        "（自由記述）\n",
        "\n",
        "課題 5\n",
        "alpha = \n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAQcs_qSkARx"
      },
      "source": [
        "<!-- ENG -->\n",
        "## Answer Sheet Template\n",
        "\n",
        "```\n",
        "Student ID number:\n",
        "Name:\n",
        "\n",
        "Exercise 1\n",
        "__xxxxx__ = __________________\n",
        "y_hat     = __________________\n",
        "\n",
        "Exercise 2\n",
        "model.coef_ = __________________\n",
        "model.intercept_ = __________________\n",
        "\n",
        "Exercise 3\n",
        "f(x) = _____x^3 + _____x^2 + _____x + _____\n",
        "(Rewrite the sign (plus/minus) as needed)\n",
        "\n",
        "Exercise 4\n",
        "(Write your answer)\n",
        "\n",
        "Exercise 5\n",
        "alpha = \n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtY8qdFUOmtA"
      },
      "source": [
        "<!-- JPN -->\n",
        "# 補足資料\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2fcdboZkARx"
      },
      "source": [
        "<!-- ENG -->\n",
        "# Supplementary Material\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLIOaO62HvCB"
      },
      "source": [
        "<!-- JPN -->\n",
        "## ※1 サンプルサイズが大きければ過剰適合は起きにくい\n",
        "\n",
        "　今回の資料では、サンプルサイズが10件のサンプル（データセット）から `sin(x)` の形状を推定することを行い、6次多項式など、高次多項式を利用すると過剰適合が発生することを確認した。\n",
        "\n",
        "　その説明の中で、「**データ数が少なく**、説明変数が多い時に過剰適合 (overfitting) が発生しやすい」と記載されていたものの、データ数を増やした場合の評価を行うことなく、Ridge回帰に話が進んでしまった。そこで、補足資料として**データ数が10件ではなく1000件**であった時に、6次多項式を用いてみる。\n",
        "\n",
        "　以下のコードを実行してみると、データ数が1000件もあると、Ridge回帰を導入するまでもなく、**3次多項式よりも6次多項式の方がよい関数推定を行うことができる**。\n",
        "`sin(x)`はテイラー展開を考えると、無限次元の多項式を用いないと完全再現できないため、データ数が十分に多ければ多項式の次数は高い方が良い関数推定を行うことができる。モデルの柔軟性（多項式回帰なら次元数 $p$ ）は、予測したい対象の関数の想定される形状と、データ数と相談しながら決定するのが望ましそうだ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf4_fZK8kARy"
      },
      "source": [
        "<!-- ENG -->\n",
        "## S1 The larger the sample size, the less likely it is that overfitting will occur\n",
        "\n",
        "　In these materials, we estimated the shape of `sin(x)` from a sample (data set) with a sample size of 10, and confirmed that overfitting occurs when higher order polynomials, such as 6th order polynomials are used.\n",
        "\n",
        "　In the explanation, it was stated that \"overfitting tends to occur when **the number of data is small** and the number of explanatory variables is large,\" but the conversation carried forward to ridge regression without evaluating the case where the number of data was increased. Therefore, as supplementary material, when **the number of data is 1000 instead of 10**, try using a 6th order polynomial.\n",
        "\n",
        "　When we run the following code, we can see that when we have 1000 data, **the 6th order polynomial gives a better function estimation than a cubic polynomial**, even without the introduction of ridge regression.\n",
        "Considering the Taylor series, `sin(x)` cannot be fully reproduced without using an infinite dimensional polynomial, so if the number of data is large enough, the higher the order of the polynomial is, the better the function estimation can be performed. The flexibility of the model (order $p$ for polynomial regression) should be determined by considering the expected function shape to be predicted and the number of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9Pz5VdTLwTc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qT9HCi5Igk-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# create toy data set\n",
        "np.random.seed(3) # A trick that allows you to create the same random data every time.\n",
        "\n",
        "n_data = 1000\n",
        "x = 6 * np.random.rand(n_data) - 3         # Randomly generate 1000 values ​​from -3 to 3\n",
        "noise = 0.1 * np.random.randn(n_data)      # Noise\n",
        "y = np.sin(x) + noise                      # Calculate y = sin (x) + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkrEp7iNJKPh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "p = 6 # Use up to the 6th order\n",
        "\n",
        "# Input make_pipeline (1st process, 2nd process, ...) in the order being processed.\n",
        "model_polyreg = make_pipeline(PolynomialFeatures(degree=p, include_bias=False),\n",
        "                              LinearRegression())\n",
        "\n",
        "# You can simply use fit() even when processing multiple calculations\n",
        "X = x[:, np.newaxis]\n",
        "model_polyreg.fit(X, y)\n",
        "draw(x, y, model_polyreg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC-su9egLzbh"
      },
      "source": [
        "<!-- JPN -->\n",
        "　実応用時には人間の行動など、関数形状が予測困難なことが多く、結局Ridge回帰などの正則化を使うことが多いが、**簡単にデータ数を増やせるのであれば、それによって予測精度を改善できることがほとんど**である（簡単にデータを取ることができないから予測したい、という欲求が生まれる訳だが…）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPT7LHakARz"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In practical applications, the function shape is often difficult to predict, such as in the example of human behavior, and norm regularization such as ridge regression is often used in the end, **but if the number of data can be easily increased, the prediction accuracy can almost always be improved by doing so**. (We can't get the data easily, which is why we want to predict...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zH538g5nNi-"
      },
      "source": [
        "<!-- JPN -->\n",
        "## ※2 標準化が必要な理由\n",
        "\n",
        "　身長と体重から何らかの値をRidge回帰を使って予測することを考える。この時、身長を[m]で表現するか、[cm]で表現するかによって、身長に対する重み $w_h$ は、100倍変化するはずである。\n",
        "\n",
        "　一方、Ridge回帰は以下の誤差関数を最小化する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz_qSem-kARz"
      },
      "source": [
        "<!-- ENG -->\n",
        "## S2 Why standardization is necessary\n",
        "\n",
        "\n",
        "\n",
        "　Consider using ridge regression to predict some value from height and weight. At this time, depending on whether the height is expressed in [m] or [cm], the weight $w_h$ for the height should change by a factor of 100.\n",
        "\n",
        "\n",
        "\n",
        "　On the other hand, ridge regression minimizes the following error function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1qZGEhWnPPX"
      },
      "source": [
        "$$\\frac{1}{n} \\sum^n_{i=1} l(f(\\boldsymbol{x}_{(i)}),y_i) + \\lambda||\\boldsymbol{w}||^2_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64PAvS8YHuSF"
      },
      "source": [
        "<!-- JPN -->\n",
        "正則化項 $\\lambda||\\boldsymbol{w}||^2_2$ を見ると、$\\boldsymbol{w}$ が100倍大きい方が正則化のペナルティが大きくなるため、身長[m]に対する重みが厳しく制限され、身長[cm]を利用した場合と異なる予測がなされてしまう。\n",
        "\n",
        "　本質的には何も差がないはずの身長の[m]と[cm]の表現で、結果を一致させるためには、説明変数を予め同じ値の幅に整えることがよく、それが標準化という操作になっているのである。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq60Dsj9kARz"
      },
      "source": [
        "<!-- ENG -->\n",
        "Looking at the norm regularization term $\\lambda||\\boldsymbol{w}||^2_2$, the penalty for norm regularization is larger when $\\boldsymbol{w}$ is 100 times larger, so the weights for height [m] are severely limited, and the prediction is different than when using height [cm].\n",
        "\n",
        "　In order to match the results in the expression of [m] and [cm] for height, which should not differ in any essential way, it is better to adjust the explanatory variables to the same range of values in advance, which is the operation of standardization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tu15QNVtwlu"
      },
      "source": [
        "<!-- JPN -->\n",
        "## ※3 予測対象に適したモデルの構築\n",
        "\n",
        "　今回対象とした $\\sin(x)$ は、テイラー展開を考えると、有限次数の多項式では厳密な表現は不可能である。そのため、（十分なデータがあるという仮定のもとで）多項式近似の次数を高めれば高めるほど予測精度は高まるはずである。一方で、今回学んだように次数を高めると過剰適合の可能性が高くなるので、多項式近似の次数はどこかで折り合いをつける必要がある。\n",
        "\n",
        "　このような場合は、**次数を上げても予測精度がほとんど向上しない場合は、なるべく次数の低いモデルを選ぶと良い**とされる（機械学習における「オッカムの剃刀」と言われることもある）。\n",
        "\n",
        "　本来、このことは基盤データサイエンスで学んだ AIC (Akaike's information criterion) や BIC (Bayesian information criterion) を導入して定量的に議論すべき点なのだが、Ridge回帰などの正則化項が加わると議論が難しくなるため、ここでは割愛する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyGrep0nkAR0"
      },
      "source": [
        "<!-- ENG -->\n",
        "## S3 Building a model that is appropriate for the prediction target\n",
        "\n",
        "　Considering the Taylor series, it is impossible to precisely represent $\\sin(x)$ we used as the target this time in the polynomial of a finite order. Therefore, the higher the order of the polynomial approximation has (under the assumption that there is enough data), the higher the prediction accuracy should be. On the other hand, as we have learned, increasing the order raises the possibility of overfitting, so the order of polynomial approximation needs to be settled somewhere.\n",
        "\n",
        "　In such cases, **when the prediction accuracy is hardly improved by increasing the order, it is better to choose a model with as low an order as possible** (sometimes referred to as \"Occam's razor\" in machine learning).\n",
        "\n",
        "　Originally, this is a point that should be discussed quantitatively by introducing AIC (Akaike's information criterion) and BIC (Bayesian information criterion), which is learned in Exercises in Fundamentals of Data Science, but it becomes complex when norm regularization terms such as ridge regression are added, so I will not discuss it here.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI3_Regression.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
